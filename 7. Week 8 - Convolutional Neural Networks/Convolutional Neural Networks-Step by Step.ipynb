{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) \n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "#ipython很好用，但是如果在ipython里已经import过的模块修改后需要重新reload就需要这样\n",
    "#在执行用户代码前，重新装入软件的扩展和模块。\n",
    "%load_ext autoreload\n",
    "#autoreload 2：装入所有 %aimport 不包含的模块。\n",
    "%autoreload 2\n",
    "np.random.seed(1)      #指定随机种子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大纲\n",
    "我们将实现一个卷积神经网络的一些模块，下面我们将列举我们要实现的模块的函数功能：\n",
    "\n",
    "- 卷积模块，包含了以下函数：\n",
    "    - 使用0扩充边界\n",
    "    - 卷积窗口\n",
    "    - 前向卷积\n",
    "    - 反向卷积（可选）\n",
    "- 池化模块，包含了以下函数：\n",
    "\n",
    "    - 前向池化\n",
    "    - 创建掩码\n",
    "    - 值分配\n",
    "    - 反向池化（可选）\n",
    "我们将在这里从底层搭建一个完整的模块，之后我们会用TensorFlow实现。模型结构如下：<br>\n",
    "\n",
    "<img src=\"images/model.png\">\n",
    "需要注意的是我们在前向传播的过程中，我们会存储一些值，以便在反向传播的过程中计算梯度值。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 卷积神经网络\n",
    "  尽管编程框架使卷积容易使用，但它们仍然是深度学习中最难理解的概念之一。卷积层将输入转换成不同维度的输出，如下所示。\n",
    "  <img src=\"images/conv_nn.png\" style=\"width:350px;height:200px;\">\n",
    "  我们将一步步构建卷积层，我们将首先实现两个辅助函数：一个用于零填充，另一个用于计算卷积。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - 边界填充\n",
    "边界填充将会在图像边界周围添加值为0的像素点，如下图所示：\n",
    "<img src=\"images/PAD.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 1** </u><font color='purple'>  : **边界填充**<br> 使用pading为2的操作对图像（3通道，RGB）进行填充。 </center></caption>\n",
    "使用0填充边界有以下好处：\n",
    "\n",
    "- 卷积了上一层之后的CONV层，没有缩小高度和宽度。 这对于建立更深的网络非常重要，否则在更深层时，高度/宽度会缩小。 一个重要的例子是“same”卷积，其中高度/宽度在卷积完一层之后会被完全保留。\n",
    "- 它可以帮助我们在图像边界保留更多信息。在没有填充的情况下，卷积过程中图像边缘的极少数值会受到过滤器的影响从而导致信息丢失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant:  \n",
      "[[[0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 1 1 2 2 3 4 0 0]\n",
      "  [0 0 1 1 2 2 3 4 0 0]\n",
      "  [0 0 1 1 2 2 3 4 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 1 2 3 4 5 0 0]\n",
      "  [0 0 0 1 2 3 4 5 0 0]\n",
      "  [0 0 0 1 2 3 4 5 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 1 1 2 2 3 4 0 0]\n",
      "  [0 0 1 1 2 2 3 4 0 0]\n",
      "  [0 0 1 1 2 2 3 4 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "#constant连续一样的值填充，有constant_values=（x, y）时前面用x填充，后面用y填充。缺省参数是为constant_values=（0,0）\n",
    "\n",
    "## a = np.pad(a,( (0,0),(1,1),(0,0),(3,3),(0,0)),'constant',constant_values = (..,..))\n",
    "\n",
    "#比如：\n",
    "import numpy as np\n",
    "arr3D = np.array([[[1, 1, 2, 2, 3, 4],\n",
    "             [1, 1, 2, 2, 3, 4], \n",
    "             [1, 1, 2, 2, 3, 4]], \n",
    "             \n",
    "            [[0, 1, 2, 3, 4, 5], \n",
    "             [0, 1, 2, 3, 4, 5], \n",
    "             [0, 1, 2, 3, 4, 5]], \n",
    "             \n",
    "            [[1, 1, 2, 2, 3, 4], \n",
    "             [1, 1, 2, 2, 3, 4], \n",
    "             [1, 1, 2, 2, 3, 4]]])\n",
    "\n",
    "print ('constant:  \\n' + str(np.pad(arr3D, ((0, 0), (1,1), (2, 2)), 'constant')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X,pad):\n",
    "    \"\"\"\n",
    "    把数据集X的图像边界全部使用0来扩充pad个宽度和高度。\n",
    "    \n",
    "    参数：\n",
    "        X - 图像数据集，维度为（样本数，图像高度，图像宽度，图像通道数）\n",
    "        pad - 整数，每个图像在垂直和水平维度上的填充量\n",
    "    返回：\n",
    "        X_paded - 扩充后的图像数据集，维度为（样本数，图像高度 + 2*pad，图像宽度 + 2*pad，图像通道数）\n",
    "    \n",
    "    \"\"\"\n",
    "    X_paded = np.pad(X,(\n",
    "                        (0,0), #样本数，不填充\n",
    "                        (pad, pad), #图像高度,你可以视为上面填充x个，下面填充y个(x,y)\n",
    "                        (pad, pad), #图像宽度,你可以视为左边填充x个，右边填充y个(x,y)\n",
    "                        (0,0)), #通道数，不填充\n",
    "                        'constant', constant_values=0) #连续一样的值填充\n",
    "     \n",
    "    return X_paded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (4, 3, 3, 2)\n",
      "x_paded.shape = (4, 7, 7, 2)\n",
      "x[1, 1] = [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_paded[1, 1] = [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24b5cac8be0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4,3,3,2)\n",
    "x_paded = zero_pad(x,2)\n",
    "#查看信息\n",
    "print (\"x.shape =\", x.shape)\n",
    "print (\"x_paded.shape =\", x_paded.shape)\n",
    "print (\"x[1, 1] =\", x[1, 1])\n",
    "print (\"x_paded[1, 1] =\", x_paded[1, 1])\n",
    "#绘制图\n",
    "fig , axarr = plt.subplots(1,2)  #一行两列\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_paded')\n",
    "axarr[1].imshow(x_paded[0,:,:,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 单步卷积\n",
    "在这里，我们要实现第一步卷积，我们要使用一个过滤器来卷积输入的数据。先来看看下面的这个:\n",
    "<img src=\"images/Convolution_schematic.gif\" style=\"width:500px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 2** </u><font color='purple'>  : **Convolution operation**<br> 过滤器大小：f = 2 ， 步伐：s = 1) </center></caption>\n",
    "在计算机视觉应用中，左侧矩阵中的每个值都对应一个像素值，我们通过将其值与原始矩阵元素相乘，然后对它们进行求和来将3x3滤波器与图像进行卷积。我们需要实现一个函数，可以将一个3x3滤波器与单独的切片块进行卷积并输出一个实数。现在我们开始实现conv_single_step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev,W,b):\n",
    "    \"\"\"\n",
    "    在前一层的激活输出的一个片段上应用一个由参数W定义的过滤器。\n",
    "    这里切片大小和过滤器大小相同\n",
    "    \n",
    "    参数：\n",
    "        a_slice_prev - 输入数据的一个片段，维度为（过滤器大小，过滤器大小，上一通道数）\n",
    "        W - 权重参数，包含在了一个矩阵中，维度为（过滤器大小，过滤器大小，上一通道数）\n",
    "        b - 偏置参数，包含在了一个矩阵中，维度为（1,1,1）\n",
    "        \n",
    "    返回：\n",
    "        Z - 在输入数据的片X上卷积滑动窗口（w，b）的结果。\n",
    "    \"\"\"\n",
    "    s = np.multiply(a_slice_prev, W) + b\n",
    "    \n",
    "    Z = np.sum(s)\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -23.16021220252078\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "#这里切片大小和过滤器大小相同\n",
    "a_slice_prev = np.random.randn(4,4,3)\n",
    "W = np.random.randn(4,4,3)\n",
    "b = np.random.randn(1,1,1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev,W,b)\n",
    "\n",
    "print(\"Z = \" + str(Z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - 卷积神经网络 - 前向传播\n",
    "在前向传播的过程中，我们将使用多种过滤器对输入的数据进行卷积操作，每个过滤器会产生一个2D的矩阵，我们可以把它们堆叠起来，于是这些2D的卷积矩阵就变成了高维的矩阵。我们可以看一下下的图：<br>\n",
    "我们需要实现一个函数以实现对激活值进行卷积。我们需要在激活值矩阵$A_prev$上使用过滤器$W$进行卷积。该函数的输入是前一层的激活输出$Aprev$, $F$个过滤器，其权重矩阵为$W$、偏置矩阵为$b$，每个过滤器只有一个偏置，最后，我们需要一个包含了步长$s$和填充$p$的字典类型的超参数。\n",
    "\n",
    "小提示：\n",
    "\n",
    "- 如果我要在矩阵A_prev（shape = (5,5,3)）的左上角选择一个2x2的矩阵进行切片操作，那么可以这样做：\n",
    "```python\n",
    "    a_slice_prev = a_prev[0:2,0:2,:],\n",
    "```\n",
    "- 如果我想要自定义切片，我们可以这么做：先定义要切片的位置，vert_start、vert_end、 horiz_start、 horiz_end，它们的位置我们看一下下面的图就明白了。\n",
    "<img src=\"images/vert_horiz_kiank.png\" style=\"width:400px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 3** </u><font color='purple'>  : **定义切片的开始、结束位置 (使用 2x2 的过滤器)** <br> This figure shows only a single channel.  </center></caption>\n",
    "\n",
    "我们还是说一下输出的维度的计算公式吧~\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$,\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$,\n",
    "$$ n_C = \\text{number of filters used in the convolution}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    实现卷积函数的前向传播\n",
    "    \n",
    "    参数：\n",
    "        A_prev - 上一层的激活输出矩阵，维度为(m, n_H_prev, n_W_prev, n_C_prev)，（样本数量，上一层图像的高度，上一层图像的宽度，上一层过滤器数量）\n",
    "        W - 权重矩阵，维度为(f, f, n_C_prev, n_C)，（过滤器大小，过滤器大小，上一层的过滤器数量，这一层的过滤器数量）\n",
    "        b - 偏置矩阵，维度为(1, 1, 1, n_C)，（1,1,1,这一层的过滤器数量）\n",
    "        hparameters - 包含了\"stride\"与 \"pad\"的超参数字典。\n",
    "    \n",
    "    返回：\n",
    "        Z - 卷积输出，维度为(m, n_H, n_W, n_C)，（样本数，图像的高度，图像的宽度，过滤器数量）\n",
    "        cache - 缓存了一些反向传播函数conv_backward()需要的一些数据\n",
    "    \"\"\"\n",
    "     #获取来自上一层数据的基本信息\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    #获取权重矩阵的基本信息\n",
    "    (f,f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    #获取超参数hparameters的值\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    n_H = int((n_H_prev - f + 2* pad)/ stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
    "    \n",
    "    #使用0来初始化卷积输出Z\n",
    "    Z = np.zeros((m,n_H,n_W,n_C))\n",
    "    \n",
    "    #通过A_prev创建填充过了的A_prev_pad\n",
    "    A_prev_pad = zero_pad(A_prev,pad)\n",
    "    \n",
    "    for i in range(m):                 #遍历样本\n",
    "        a_prev_pad = A_prev_pad[i]      #选择第i个样本的扩充后的激活矩阵\n",
    "        for h in range(n_H):            #在输出的垂直轴上循环\n",
    "            for w in range(n_W):        #在输出的水平轴上循环\n",
    "                for c in range(n_C):    #循环遍历输出的通道\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    hori_start = w* stride\n",
    "                    hori_end = hori_start + f\n",
    "                    #切片位置定位好了我们就把它取出来,需要注意的是我们是“穿透”取出来的，\n",
    "                    #自行脑补一下吸管插入一层层的橡皮泥就明白了\n",
    "                    a_slice_prev = a_prev_pad[vert_start: vert_end, hori_start: hori_end, :]\n",
    "                     #执行单步卷积\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:, :, :, c], b[0,0,0,c])\n",
    "    #数据处理完毕，验证数据格式是否正确\n",
    "    assert(Z.shape == (m , n_H , n_W , n_C ))\n",
    "    \n",
    "    #存储一些缓存值，以便于反向传播使用\n",
    "    cache = (A_prev,W,b,hparameters)\n",
    "    \n",
    "    return (Z , cache)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.mean(Z) =  0.15585932488906465\n",
      "cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "\n",
    "hparameters = {\"pad\" : 2, \"stride\": 1}\n",
    "\n",
    "Z , cache_conv = conv_forward(A_prev,W,b,hparameters)\n",
    "\n",
    "print(\"np.mean(Z) = \", np.mean(Z))\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，卷积层应该包含一个激活函数，我们可以加一行代码来计算:\n",
    "\n",
    "```python\n",
    "#获取输出\n",
    "Z[i, h, w, c] = ...\n",
    "#计算激活\n",
    "A[i, h, w, c] = activation(Z[i, h, w, c])\n",
    "```    \n",
    "You don't need to do it here.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - 池化层<br>\n",
    "\n",
    "  池化层会减少输入的宽度和高度，这样它会较少计算量的同时也使特征检测器对其在输入中的位置更加稳定。下面介绍两种类型的池化层：\n",
    "- 最大值池化层：在输入矩阵中滑动一个大小为fxf的窗口，选取窗口里的值中的最大值，然后作为输出的一部分。\n",
    "- 均值池化层：在输入矩阵中滑动一个大小为fxf的窗口，计算窗口里的值中的平均值，然后这个均值作为输出的一部分。\n",
    "<table>\n",
    "<td>\n",
    "   <img src=\"images/max_pool1.png\">\n",
    "   <td>\n",
    "   <td>\n",
    "   <img src=\"images/a_pool.png\">\n",
    "   <td>\n",
    " </table>\n",
    " 池化层没有用于进行反向传播的参数，但是它们有像窗口的大小为$f$的超参数，它指定fxf窗口的高度和宽度，我们可以计算出最大值或平均值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 - 池化层的前向传播\n",
    "\n",
    "现在我们要在同一个函数中实现```最大值池化层```和```均值池化层```，和之前计算输出维度一样，池化层的计算也是一样的。\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f}{stride} \\rfloor +1 $$,\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f}{stride} \\rfloor +1 $$,\n",
    "$$ n_C = n_{C_{prev}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev,hparameters,mode=\"max\"):\n",
    "    \"\"\"\n",
    "    实现池化层的前向传播\n",
    "    \n",
    "    参数：\n",
    "        A_prev - 输入数据，维度为(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        hparameters - 包含了 \"f\" 和 \"stride\"的超参数字典\n",
    "        mode - 模式选择【\"max\" | \"average\"】\n",
    "        \n",
    "    返回：\n",
    "        A - 池化层的输出，维度为 (m, n_H, n_W, n_C)\n",
    "        cache - 存储了一些反向传播需要用到的值，包含了输入和超参数的字典。\n",
    "    \"\"\"\n",
    "    \n",
    "    #获取输入数据的基本信息\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    #获取超参数的信息\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    #计算输出维度\n",
    "    n_H = int((n_H_prev - f) / stride) + 1\n",
    "    n_W = int((n_W_prev - f) / stride) + 1\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    #初始化输出矩阵\n",
    "    A = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    for i in range(m):                           #遍历样本\n",
    "        for h in range(n_H):                     #在输出的垂直轴上循环\n",
    "            for w in range(n_W):                 #在输出的水平轴上循环\n",
    "                for c in range(n_C):             #循环遍历输出的通道\n",
    "                    #定位当前的切片位置\n",
    "                    vert_start = h * stride      #竖向，开始的位置\n",
    "                    vert_end = vert_start + f    #竖向，结束的位置\n",
    "                    hori_start = w * stride      #横向，开始的位置\n",
    "                    hori_end = hori_start + f    #横向，结束的位置\n",
    "                    #定位完毕，开始切割\n",
    "                    a_slice_prev = A_prev[i, vert_start:vert_end, hori_start: hori_end, c]\n",
    "                    \n",
    "                    #对切片进行池化操作\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_slice_prev)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_slice_prev)\n",
    "    #池化完毕， 校验数据格式\n",
    "    assert(A.shape == (m , n_H , n_W , n_C))\n",
    "    \n",
    "    #校验完毕，开始存储用于反向传播的值\n",
    "    cache = (A_prev,hparameters)\n",
    "    \n",
    "    return A,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A = [[[[1.74481176 1.6924546  2.10025514]]]\n",
      "\n",
      "\n",
      " [[[1.19891788 1.51981682 2.18557541]]]]\n",
      "----------------------------\n",
      "mode = average\n",
      "A = [[[[-0.09498456  0.11180064 -0.14263511]]]\n",
      "\n",
      "\n",
      " [[[-0.09525108  0.28325018  0.33035185]]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2,4,4,3)\n",
    "hparameters = {\"f\":4 , \"stride\":1}\n",
    "\n",
    "A , cache = pool_forward(A_prev,hparameters,mode=\"max\")\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A =\", A)\n",
    "print(\"----------------------------\")\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A =\", A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 - 卷积神经网络中的反向传播\n",
    "在现在的深度学习框架中，你只需要实现前向传播，框架负责向后传播，所以大多数深度学习工程师不需要费心处理后向传播的细节，卷积网络的后向传递是有点复杂的。但是如果你愿意，你可以选择性来学习本节。\n",
    "\n",
    "  在前面的课程中，我们已经实现了一个简单的(全连接)神经网络，我们使用反向传播来计算关于更新参数的成本的梯度。类似地，在卷积神经网络中我们可以计算出关于成本的导数来更新参数。反向传播的方程并不简单，吴恩达老师并没有在课堂上推导它们，但我们可以在下面简要介绍。\n",
    "## 1.5.1 - 卷积层的反向传播\n",
    "我们来看一下如何实现卷积层的反向传播\n",
    "## 1.5.1.1 - 计算dA\n",
    "下面的公式是计算$dA$的：\n",
    "$$ dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \\tag{1}$$\n",
    "其中，$W_c$是过滤器，$Z_{hw}$是一个标量，$Z_{hw}$是卷积层第$h$行第$w$列的使用点乘计算后的输出$Z$的梯度。需要注意的是在每次更新$dA$的时候，都会用相同的过滤器$W_c$乘以不同的$dZ$,因为在前向传播的时候，每个过滤器都与a_slice进行了点乘相加，所以在计算$dA$的时候，我们需要把a_slice的梯度也加进来，我们可以在循环中加一句代码：\n",
    "```python\n",
    "da_perv_pad[vert_start:vert_end,horiz_start:horiz_end,:] += W[:,:,:,c] * dZ[i,h,w,c]\n",
    "```\n",
    "## 1.5.1.2 - 计算dW\n",
    "这是计算$dW_c$的公式（$dW_c$是一个过滤器的梯度）：\n",
    "$$ dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}$$\n",
    "其中，$a_{slice}$对应着$Z_{ij}$的激活值。由此，我们就可以推导$W$的梯度，因为我们使用了过滤器来对数据进行窗口滑动，在这里，我们实际上是切出了和过滤器一样大小的切片，切了多少次就产生了多少个梯度，所以我们需要把它们加起来得到这个数据集的整体$dW$。\n",
    "在代码上我们只需要使用一行代码实现：\n",
    "```python\n",
    "dW[:,:,:, c] += a_slice * dZ[i , h , w , c]\n",
    "\n",
    "```\n",
    "## 1.5.1.3 - 计算db\n",
    "这个是计算$db$的公式：\n",
    "$$ db = \\sum_h \\sum_w dZ_{hw} \\tag{3}$$\n",
    "和以前的神经网络一样，$db$是由$dZ$的累加计算的，在这里，我们只需要将conv的输出$Z$的所有梯度累加就好了。在代码上我们只需要使用一行代码实现：\n",
    "```python\n",
    "db[:,:,:,c] += dZ[ i, h, w, c]\n",
    "\n",
    "```\n",
    "\n",
    "## 1.5.1.4 - 函数实现\n",
    "  现在我们将实现反向传播函数conv_backward()，我们需要把所有的训练样本的过滤器、权值、高度、宽度都要加进来，然后使用公式1、2、3计算对应的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ,cache):\n",
    "    \"\"\"\n",
    "    实现卷积层的反向传播\n",
    "    \n",
    "    参数：\n",
    "        dZ - 卷积层的输出Z的 梯度，维度为(m, n_H, n_W, n_C)\n",
    "        cache - 反向传播所需要的参数，conv_forward()的输出之一\n",
    "        \n",
    "    返回：\n",
    "        dA_prev - 卷积层的输入（A_prev）的梯度值，维度为(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        dW - 卷积层的权值的梯度，维度为(f,f,n_C_prev,n_C)\n",
    "        db - 卷积层的偏置的梯度，维度为（1,1,1,n_C）\n",
    "    \n",
    "    \"\"\"\n",
    "    #获取cache的值\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "\n",
    "    #获取A_prev的基本信息\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    #获取dZ的基本信息\n",
    "    (m,n_H,n_W,n_C) = dZ.shape\n",
    "     \n",
    "    #获取权值的基本信息\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    #获取hparaeters的值\n",
    "    pad = hparameters[\"pad\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    #初始化各个梯度的结构\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    dW = np.zeros((f,f,n_C_prev, n_C))\n",
    "    db = np.zeros((1,1,1,n_C))\n",
    "    \n",
    "    #前向传播中我们使用了pad，反向传播也需要使用，这是为了保证数据结构一致\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    #现在处理数据\n",
    "    for i in range(m):\n",
    "        #选择第i个扩充了的数据的样本,降了一维。\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    #定位切片位置\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    #定位完毕，开始切片\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    #切片完毕，使用上面的公式计算梯度\n",
    "                    da_prev_pad[vert_start:vert_end,horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i,h,w,c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i,h,w,c]\n",
    "                    db[:,:,:,c] += dZ[i,h,w,c]\n",
    "         #设置第i个样本最终的dA_prev,即把非填充的数据取出来。\n",
    "        dA_prev[i,:,:,:] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    \n",
    "    #数据处理完毕，验证数据格式是否正确\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return (dA_prev,dW,db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 9.608990675868995\n",
      "dW_mean = 10.581741275547566\n",
      "db_mean = 76.37106919563735\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "#初始化参数\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2, \"stride\": 1}\n",
    "\n",
    "#前向传播\n",
    "Z , cache_conv = conv_forward(A_prev,W,b,hparameters)\n",
    "#反向传播\n",
    "dA , dW , db = conv_backward(Z,cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5.2 - 池化层的反向传播\n",
    "  接下来，我们从最大值池化层开始实现池化层的反向传播。 即使池化层没有反向传播过程中要更新的参数，我们仍然需要通过池化层反向传播梯度，以便为在池化层之前的层（比如卷积层）计算梯度。\n",
    "## 1.5.2.1 最大值池化层的反向传播\n",
    "在开始池化层的反向传播之前，我们需要创建一个create_mask_from_window()的函数，我们来看一下它是干什么的：\n",
    "$$ X = \\begin{bmatrix}\n",
    "    1 && 3 \\\\\n",
    "  4 && 2 \\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}0 && 0 \\\\\n",
    "    1 && 0   \\end{bmatrix}\\tag{4}$$\n",
    "    \n",
    "正如你所看到的，这个函数创建了一个掩码矩阵，以保存最大值的位置，当为1的时候表示最大值的位置，其他的为0，这个是最大值池化层，均值池化层的向后传播也和这个差不多，但是使用的是不同的掩码。\n",
    "- np.max() may be helpful. It computes the maximum of an array.\n",
    "- If you have a matrix X and a scalar x: A = (X == x) will return a matrix A of the same size as X such that: \n",
    "```python\n",
    "A[i,j] = True if X[i,j] = x\n",
    "A[i,j] = False if X[i,j] != x\n",
    "```\n",
    "- Here, you don't need to consider cases where there are several maxima in a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    从输入矩阵中创建掩码，以保存最大值的矩阵的位置。\n",
    "    \n",
    "    参数：\n",
    "        x - 一个维度为(f,f)的矩阵\n",
    "        \n",
    "    返回：\n",
    "        mask - 包含x的最大值的位置的矩阵\n",
    "    \"\"\"\n",
    "    mask = x == np.max(x)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask = [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "x = np.random.randn(2,3)\n",
    "\n",
    "mask = create_mask_from_window(x)\n",
    "\n",
    "print(\"x = \" + str(x)) \n",
    "print(\"mask = \" + str(mask))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we keep track of the position of the max? It's because this is the input value that ultimately influenced the output, and therefore the cost. Backprop is computing gradients with respect to the cost, so anything that influences the ultimate cost should have a non-zero gradient. So, backprop will \"propagate\" the gradient back to this particular input value that had influenced the cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5.2.2 均值池化层的反向传播\n",
    "在最大值池化层中，对于每个输入窗口，输出的所有值都来自输入中的最大值，但是在均值池化层中，因为是计算均值，所以输入窗口的每个元素对输出有一样的影响，我们来看看如何反向传播吧\n",
    "In max pooling, for each input window, all the \"influence\" on the output came from a single input value--the max. In average pooling, every element of the input window has equal influence on the output. So to implement backprop, you will now implement a helper function that reflects this.\n",
    "\n",
    "For example if we did average pooling in the forward pass using a 2x2 filter, then the mask you'll use for the backward pass will look like:\n",
    "$$ dZ = 1 \\quad \\rightarrow  \\quad dZ =\\begin{bmatrix}\n",
    "    1/4 && 1/4 \\\\\n",
    "    1/4 && 1/4\n",
    "    \\end{bmatrix}\\tag{5}$$\n",
    "This implies that each position in the $dZ$ matrix contributes equally to output because in the forward pass, we took an average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz,shape):\n",
    "    \"\"\"\n",
    "    给定一个值，为按矩阵大小平均分配到每一个矩阵位置中。\n",
    "    \n",
    "    参数：\n",
    "        dz - 输入的实数\n",
    "        shape - 元组，两个值，分别为n_H , n_W\n",
    "        \n",
    "    返回：\n",
    "        a - 已经分配好了值的矩阵，里面的值全部一样。\n",
    "    \n",
    "    \"\"\"\n",
    "    #获取矩阵的大小\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    #计算平均值\n",
    "    average = dz / (n_H * n_W)\n",
    "    \n",
    "    #填充入矩阵\n",
    "    a = np.ones(shape) * average\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = [[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "dz = 2\n",
    "shape = (2,2)\n",
    "\n",
    "a = distribute_value(dz,shape)\n",
    "print(\"a = \" + str(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA,cache,mode = \"max\"):\n",
    "    \"\"\"\n",
    "    实现池化层的反向传播\n",
    "    \n",
    "    参数:\n",
    "        dA - 池化层的输出的梯度，和池化层的输出的维度一样\n",
    "        cache - 池化层前向传播时所存储的参数。\n",
    "        mode - 模式选择，【\"max\" | \"average\"】\n",
    "        \n",
    "    返回：\n",
    "        dA_prev - 池化层的输入的梯度，和A_prev的维度相同\n",
    "    \n",
    "    \"\"\"\n",
    "    #获取cache的值\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    #湖片区hparameters的值\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    #获取A_prev和dA的基本信息\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (m, n_H, n_W, n_C) = dA.shape\n",
    "    \n",
    "    #初始化输出的结构\n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "    \n",
    "    #开始处理数据\n",
    "    for i in range(m):\n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    #定位切片位置\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        #开始切片\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        #创建掩码\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        #计算dA_prev\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i,h,w,c])\n",
    "                    elif mode == \"average\":\n",
    "                        #获取dA的值\n",
    "                        da = dA[i,h,w,c]\n",
    "                        #定义过滤器大小\n",
    "                        shape = (f, f)\n",
    "                        #平均分配\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "\n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1]) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
